close

Open settings <#>
close

Google Account
Tes Ùs
gtmeraj16@gmail.com
This notebook is open with private outputs. Outputs will not be saved.
You can disable this in Notebook settings <#>.

<https://drive.google.com/drive/search?
q=owner%3Ame%20(type%3Aapplication%2Fvnd.google.colaboratory%20%7C%7C%20type%3Aapplication%2Fvnd.google.colab)&authuser=0>

Session.ipynb_

star

File
 
Edit
 
View
 
Insert
 
Runtime
 
Tools
 
Help
 

comment

settings

people

Share

<https://accounts.google.com/SignOutOptions?hl=en&continue=https://
colab.research.google.com/drive/14fuRYVDpPg_ZUNyddZmAyEADRWCSd-
j2&ec=GBRAqQM>
format_list_bulleted

search

vpn_key

folder

code

terminal

Code

Text

Gemini

people

settings

expand_less

expand_more

Notebook

more_horiz

------------------------------------------------------------------------
keyboard_arrow_down


  _AI Powered Language Processing _

↳ 0 cells hidden
------------------------------------------------------------------------
keyboard_arrow_down


  Session: Text classification using ML


      Objective:

 1. /Apply Tf-idf./
 2. /Apply LR Algorithm./

↳ 3 cells hidden
------------------------------------------------------------------------

import warnings
warnings.filterwarnings("ignore")

------------------------------------------------------------------------

import sklearn
print(sklearn.__version__)

1.5.2

add

Code

add

Text

------------------------------------------------------------------------

%%time
import os
from glob import glob
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
import pandas as pd
import seaborn as sns
import re
import nltk
import json
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression, LogisticRegressionCV
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score,roc_auc_score
from sklearn.metrics import average_precision_score,roc_auc_score, roc_curve, precision_recall_curve
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer
np.random.seed(42)

CPU times: user 3.87 s, sys: 382 ms, total: 4.25 s
Wall time: 11.9 s

add

Code

add

Text

------------------------------------------------------------------------
keyboard_arrow_down


  Dataset Fetching

↳ 13 cells hidden
------------------------------------------------------------------------

# Download the data
!wget -O reviews.csv https://www.dropbox.com/s/vyhodtbbumedlv2/restaurant-reviews.csv?dl=0

--2024-11-18 16:06:49--  https://www.dropbox.com/s/vyhodtbbumedlv2/restaurant-reviews.csv?dl=0 <https://www.dropbox.com/s/vyhodtbbumedlv2/restaurant-reviews.csv?dl=0>
Resolving www.dropbox.com <http://www.dropbox.com/> (www.dropbox.com <http://www.dropbox.com/>)... 162.125.3.18, 2620:100:6018:18::a27d:312
Connecting to www.dropbox.com <http://www.dropbox.com/> (www.dropbox.com)|162.125.3.18|:443 <http://www.dropbox.com)|162.125.3.18|:443>... connected.
HTTP request sent, awaiting response... 302 Found
Location: https://www.dropbox.com/scl/fi/2ca73mc0dykpss9phrj8t/restaurant-reviews.csv?rlkey=t4stjed1v7e9e6h5hucqmvdha&dl=0 <https://www.dropbox.com/scl/fi/2ca73mc0dykpss9phrj8t/restaurant-reviews.csv?rlkey=t4stjed1v7e9e6h5hucqmvdha&dl=0> [following]
--2024-11-18 16:06:49--  https://www.dropbox.com/scl/fi/2ca73mc0dykpss9phrj8t/restaurant-reviews.csv?rlkey=t4stjed1v7e9e6h5hucqmvdha&dl=0 <https://www.dropbox.com/scl/fi/2ca73mc0dykpss9phrj8t/restaurant-reviews.csv?rlkey=t4stjed1v7e9e6h5hucqmvdha&dl=0>
Reusing existing connection to www.dropbox.com:443 <http://www.dropbox.com:443/>.
HTTP request sent, awaiting response... 302 Found
Location: https://uce1621bff855391a481d8594161.dl.dropboxusercontent.com/cd/0/inline/CenwwanzbQzP2UzvT40Y0gaOTCqtnULoV6tdxeQmxYIDvYozNIAQXiNevcORw7ivK43RZSpQcLFDqOW4TzmboLriz1qtmFkFmaMRsTSrPn4ajUGt7U8fZ1qyDVu9iv2XBdLSlEDPzVSjhh49g9wRe_Kz/file# <https://uce1621bff855391a481d8594161.dl.dropboxusercontent.com/cd/0/inline/CenwwanzbQzP2UzvT40Y0gaOTCqtnULoV6tdxeQmxYIDvYozNIAQXiNevcORw7ivK43RZSpQcLFDqOW4TzmboLriz1qtmFkFmaMRsTSrPn4ajUGt7U8fZ1qyDVu9iv2XBdLSlEDPzVSjhh49g9wRe_Kz/file#> [following]
--2024-11-18 16:06:50--  https://uce1621bff855391a481d8594161.dl.dropboxusercontent.com/cd/0/inline/CenwwanzbQzP2UzvT40Y0gaOTCqtnULoV6tdxeQmxYIDvYozNIAQXiNevcORw7ivK43RZSpQcLFDqOW4TzmboLriz1qtmFkFmaMRsTSrPn4ajUGt7U8fZ1qyDVu9iv2XBdLSlEDPzVSjhh49g9wRe_Kz/file <https://uce1621bff855391a481d8594161.dl.dropboxusercontent.com/cd/0/inline/CenwwanzbQzP2UzvT40Y0gaOTCqtnULoV6tdxeQmxYIDvYozNIAQXiNevcORw7ivK43RZSpQcLFDqOW4TzmboLriz1qtmFkFmaMRsTSrPn4ajUGt7U8fZ1qyDVu9iv2XBdLSlEDPzVSjhh49g9wRe_Kz/file>
Resolving uce1621bff855391a481d8594161.dl.dropboxusercontent.com (uce1621bff855391a481d8594161.dl.dropboxusercontent.com)... 162.125.3.15, 2620:100:6018:15::a27d:30f
Connecting to uce1621bff855391a481d8594161.dl.dropboxusercontent.com (uce1621bff855391a481d8594161.dl.dropboxusercontent.com)|162.125.3.15|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 496798 (485K) [text/plain]
Saving to: ‘reviews.csv’

reviews.csv         100%[===================>] 485.15K  --.-KB/s    in 0.09s   

2024-11-18 16:06:50 (5.56 MB/s) - ‘reviews.csv’ saved [496798/496798]

add

Code

add

Text

------------------------------------------------------------------------

# %%time
data = pd.read_csv("reviews.csv")
data = data.dropna()

add

Code

add

Text

------------------------------------------------------------------------

tprint("Size of the total data==> ", len(data))
class_names= data.Sentiment.unique() ## Get the unique values in a specific column
print(class_names)

Size of the total data==>  1431
['positive' 'negative']

------------------------------------------------------------------------

data['training_label']=data.Sentiment.replace({'positive':1,'negative':0})
data.sample(20)

add

Code

add

Text

------------------------------------------------------------------------

##some bangla examples##
def cleaning_bangla(x):
    if not isinstance(x, str):
        return x  # Return as is if not a string
    # Remove Bangla punctuation
    x = re.sub(r'[।,!?]', '', x)
    # Remove extra spaces
    x = re.sub(r'\s+', ' ', x).strip()
    # Optional: Convert Bangla numerals to English
    bangla_to_english = str.maketrans('০১২৩৪৫৬৭৮৯', '0123456789')
    x = x.translate(bangla_to_english)
    return x
# Example usage:
text = "আমার সোনার বাংলা। ১২৩!"
cleaned_text = cleaning_bangla(text)
print(cleaned_text)  # Output: "আমার সোনার বাংলা 123"

##some bangla examples## def cleaning_bangla(x): if not isinstance(x,
str): return x # Return as is if not a string # Remove Bangla
punctuation x = re.sub(r'[।,!?]', '', x) # Remove extra spaces

Loading...

------------------------------------------------------------------------

%%time
# text cleaning

def cleaning_bangla(x):
  x = re.sub('[^\u0980-\u09FF]',' ', x)
  # Remove quotes from source text
  x = re.sub("'", '', x)
  # create a set of all special characters
  x = x.strip()
  return x

data['cleaned'] = data['Reviews'].apply(cleaning_bangla)

CPU times: user 27.7 ms, sys: 2.84 ms, total: 30.6 ms
Wall time: 35.3 ms

------------------------------------------------------------------------

data.head(15)

------------------------------------------------------------------------
keyboard_arrow_down


    Visualize tf-idf and count vectorizer

↳ 5 cells hidden
------------------------------------------------------------------------

tf_idf = TfidfVectorizer(ngram_range=(1,1), min_df=1, tokenizer=lambda x: x.split(),  use_idf = True)
tf_corpus = tf_idf.fit_transform(data.cleaned[:3])
cv = CountVectorizer(ngram_range=(1,1), min_df=1, tokenizer=lambda x: x.split())
cv_corpus = cv.fit_transform(data.cleaned[:3])

------------------------------------------------------------------------

#Here first parameter is the number of documents and second parameter is the number of features

print(tf_corpus.shape, cv_corpus.shape)

print(cv.vocabulary_)

print(cv.get_feature_names_out())

(3, 31) (3, 31)
{'পিজাটা': 23, 'চমৎকার': 16, 'ছিল': 17, 'আমি': 3, 'এবং': 9, 'আমার': 2, 'বন্ধুদের': 25, 'এটি': 7, 'পছন্দ': 22, 'হয়েছে': 30, 'এরাবিয়ান': 10, 'মাস্টারের': 27, 'অভ্যন্তরীণ': 1, 'সজ্জাটা': 28, 'অনন্য': 0, 'এটির': 8, 'খাদ্য': 14, 'সুস্বাদু': 29, 'একটু': 6, 'মসলাযুক্ত': 26, 'তাদের': 21, 'কর্মীরা': 12, 'খুব': 15, 'বন্ধুত্বপূর্ণ': 24, 'কোরিয়ান': 13, 'জামাইকান': 19, 'উপভোগ': 4, 'করার': 11, 'জন্য': 18, 'একটি': 5, 'জায়গা': 20}
['অনন্য' 'অভ্যন্তরীণ' 'আমার' 'আমি' 'উপভোগ' 'একটি' 'একটু' 'এটি' 'এটির'
 'এবং' 'এরাবিয়ান' 'করার' 'কর্মীরা' 'কোরিয়ান' 'খাদ্য' 'খুব' 'চমৎকার' 'ছিল'
 'জন্য' 'জামাইকান' 'জায়গা' 'তাদের' 'পছন্দ' 'পিজাটা' 'বন্ধুত্বপূর্ণ'
 'বন্ধুদের' 'মসলাযুক্ত' 'মাস্টারের' 'সজ্জাটা' 'সুস্বাদু' 'হয়েছে']

------------------------------------------------------------------------

# Convert sparse matrix to dataframe
cv_corpus = pd.DataFrame.sparse.from_spmatrix(cv_corpus)
# Save mapping on which index refers to which words
col_map = {v:k for k, v in cv.vocabulary_.items()}
# Rename each column using the mapping
for col in cv_corpus.columns:
    cv_corpus.rename(columns={col: col_map[col]}, inplace=True)
cv_corpus

------------------------------------------------------------------------

print(tf_idf.vocabulary_) ## Returns the vocabulary with index

print(tf_idf.get_feature_names_out()) ## Return the feature words

{'পিজাটা': 23, 'চমৎকার': 16, 'ছিল': 17, 'আমি': 3, 'এবং': 9, 'আমার': 2, 'বন্ধুদের': 25, 'এটি': 7, 'পছন্দ': 22, 'হয়েছে': 30, 'এরাবিয়ান': 10, 'মাস্টারের': 27, 'অভ্যন্তরীণ': 1, 'সজ্জাটা': 28, 'অনন্য': 0, 'এটির': 8, 'খাদ্য': 14, 'সুস্বাদু': 29, 'একটু': 6, 'মসলাযুক্ত': 26, 'তাদের': 21, 'কর্মীরা': 12, 'খুব': 15, 'বন্ধুত্বপূর্ণ': 24, 'কোরিয়ান': 13, 'জামাইকান': 19, 'উপভোগ': 4, 'করার': 11, 'জন্য': 18, 'একটি': 5, 'জায়গা': 20}
['অনন্য' 'অভ্যন্তরীণ' 'আমার' 'আমি' 'উপভোগ' 'একটি' 'একটু' 'এটি' 'এটির'
 'এবং' 'এরাবিয়ান' 'করার' 'কর্মীরা' 'কোরিয়ান' 'খাদ্য' 'খুব' 'চমৎকার' 'ছিল'
 'জন্য' 'জামাইকান' 'জায়গা' 'তাদের' 'পছন্দ' 'পিজাটা' 'বন্ধুত্বপূর্ণ'
 'বন্ধুদের' 'মসলাযুক্ত' 'মাস্টারের' 'সজ্জাটা' 'সুস্বাদু' 'হয়েছে']

------------------------------------------------------------------------

# Convert sparse matrix to dataframe
tf_corpus = pd.DataFrame.sparse.from_spmatrix(tf_corpus)
# Save mapping on which index refers to which words
col_map = {v:k for k, v in tf_idf.vocabulary_.items()}
# Rename each column using the mapping
for col in tf_corpus.columns:
    tf_corpus.rename(columns={col: col_map[col]}, inplace=True)
tf_corpus

------------------------------------------------------------------------
keyboard_arrow_down


  Model Training

↳ 6 cells hidden
------------------------------------------------------------------------

corpus, labels = data.cleaned, data.training_label

------------------------------------------------------------------------

X_train, X_test, y_train, y_test = train_test_split(corpus,labels,train_size = 0.8, test_size = 0.2,random_state =0)

------------------------------------------------------------------------

from sklearn.pipeline import Pipeline, make_pipeline

------------------------------------------------------------------------

%%time
vec = TfidfVectorizer(ngram_range=(1,1), min_df=1, tokenizer=lambda x: x.split(), use_idf=True)
# vec = CountVectorizer(ngram_range=(1,1), min_df=1, tokenizer=lambda x: x.split())
clf = MultinomialNB()
pipe = make_pipeline(vec, clf)
pipe.fit(X_train, y_train)

------------------------------------------------------------------------

from sklearn import metrics
def print_report(pipe):
    y_pred = pipe.predict(X_test)
    report = metrics.classification_report(y_test, y_pred,
        target_names=['negative','positive'])
    cm= confusion_matrix(y_test, y_pred)
    print(report)
    print(cm)
    print("accuracy: {:0.3f}".format(metrics.accuracy_score(y_test, y_pred)))

print_report(pipe)

              precision    recall  f1-score   support

    negative       0.81      0.91      0.86       155
    positive       0.88      0.75      0.81       132

    accuracy                           0.84       287
   macro avg       0.84      0.83      0.83       287
weighted avg       0.84      0.84      0.83       287

[[141  14]
 [ 33  99]]
accuracy: 0.836

------------------------------------------------------------------------

def print_prediction(doc):
    y_pred = pipe.predict_proba([doc])[0]
    for target, prob in zip(['negative','positive'], y_pred):
        print("{:.3f} {}".format(prob, target))

doc = data.cleaned[1]
print(doc)
print_prediction(doc)

এরাবিয়ান মাস্টারের  অভ্যন্তরীণ সজ্জাটা অনন্য  এটির খাদ্য সুস্বাদু এবং একটু মসলাযুক্ত    তাদের কর্মীরা খুব বন্ধুত্বপূর্ণ
0.254 negative
0.746 positive

------------------------------------------------------------------------
keyboard_arrow_down


  Model Explanation

↳ 1 cell hidden
------------------------------------------------------------------------

%%time
vec = TfidfVectorizer(ngram_range=(1,1), min_df=1, tokenizer=lambda x: x.split(), use_idf=True)
# vec = CountVectorizer(ngram_range=(1,1), min_df=1, tokenizer=lambda x: x.split())
clf = LogisticRegressionCV()
pipe = make_pipeline(vec, clf)
pipe.fit(X_train, y_train)

------------------------------------------------------------------------
Colab paid products <https://colab.research.google.com/signup?
utm_source=footer&utm_medium=link&utm_campaign=footer_links> - Cancel
contracts here <https://colab.research.google.com/cancel-subscription>

more_horiz

more_horiz

more_horiz

Locate in Drive
Open in playground mode
New notebook in Drive
Open notebookCtrl+O
Upload notebook
Rename
Move
Move to trash
Save a copy in Drive
Save a copy as a GitHub Gist
Save a copy in GitHub
SaveCtrl+S
Save and pin revisionCtrl+M S
Revision history
Download ►
PrintCtrl+P
Download .ipynb
Download .py
Undo insert cellCtrl+M Z
RedoCtrl+Shift+Y
Select all cellsCtrl+Shift+A
Cut cell or selection
Copy cell or selection
Paste
Delete selected cellsCtrl+M D
Find and replaceCtrl+H
Find nextCtrl+G
Find previousCtrl+Shift+G
Notebook settings
Clear all outputs
check
Table of contents
Notebook info
Executed code history
check
Comments sidebar
Collapse sections
Expand sections
Save collapsed section layout
Show/hide code
Show/hide output
Focus next tab
Focus previous tab
Move tab to next pane
Move tab to previous pane
Code cell
Text cell
Section header cell
Scratch code cell
Code snippets
Add a form field
Run all
Run before
Run the focused cell
Run selection
Run cell and below
Interrupt execution
Restart session
Restart session and run all
Disconnect and delete runtime
Change runtime type
Manage sessions
View resources
View runtime logs
Command palette
Settings
Keyboard shortcuts
Diff notebooks (opens in a new tab)
Frequently asked questions
View release notes
Search code snippets
Report a bug
Report Drive abuse
Send feedback
View terms of service
comment

Add a comment
data.sample(10)
03AFcWeA6wshKrxHi-7lge9C8HZYayTkI-unD0zOw2ztB6mHYeZajDgjB8cEyc5-ChrzK-
vqU2z-hAHUNjY7L7yquLQuAAU955gdV7YBC8bffqO1WD2F8dQgppRNt_ETsRcaVS-
ARwgmd5f5bPjYRYDC2wySdmVl3JYGxpfV3g0t_cFAYHHThys3ePATBXUKTH-
jr4c0gYpgBj60j64WAJ-
pe3b146eeNTzKP74lIYYoT2iFZk85gfELyiNtyhDQpyeCj0eoTv5ixqLIazB9634_ctBU5l7i7GnF8IYIsxYFXW6OlHzmFKOYDXew0BCZejniq1sGaplKaKp_ObAQPPv7SKtWdduMi_ouQ5h8bgznFq4crMnsISktXwooyvHzbIQaoANrTTaFnv_ekmUcIt5q6G47DTMVgshwVNoVW0bFrQuAAGqnk9pok92snaRWQUXql69Qs-8FeDFy_SMwzfb8YRtz0_VqJcvrR48-ueUsHVyyHG5RJnuEzjCJPGL9OoMaoaJdrx6QfZwSQlpkLirlgsGkjfAmHjHDU0QtU6C5IWSuZmfRsVkTfHPgD8AGO2x5o1ojrupQr43k6jZT44yNMafF5VtyMRvi3VAlLiClmqHYW2vdRWblKznhiTeRuvajNmlniXJdimwTzv3RBGnvrUlEUhnF2mLfUXj7t779enzzVM5rRc2Q_UV0deubcP6ODljEn2RRYLm1h6lhGFPsDLMUzRwFbUSue707VdjzBWebZIwevx-mVZlWYid1VEk3DJSrf6CrWUCtxjmik8or1cR4wJWvt1x-SwgAgdCGhceIYs7icwu7pZNe08QAAI_b9HL8Vw1Qm2F9EmwHaTJEYF4V8Gy1b8vf7QnchMhUale5kT0Jf_y4K893854Xj7bP0NE3FzqcaYigJfVT85e4o4dgSutWKpwSlhOd1z5uwtONeCATeU_hGdkLepf3X6Ld8SSUE_dOQLum_AdirAixV9Ftqd0m2a5-k5j5Fw76WVrZvxliJWr75j-8SYZUUcoSAEvgtn7YDPh02WClu8mqFviy6D9ks8g7NrRR-alopCIFQhLiGo2iY7sb0EMzEeM5fkJnghnYFK722H4bcOpHSfaCz5pBMI6lWBOrDDvz3JRcZInfpDS-iOggd2cZRICyEW9unsrtMWLDYgfuoKpUg3U71piKbtpwTTSa4IAOvT0t8jiBID0sf9zUvqhNFpVAcfV5DDDsWBXWcg7FvFrpWwpHyC4LbUBiCupeY1pbrHxfNC0HalQCAfZHYtT57kPZzwVZtSsEMXOQdjI8HwWpqVNZZFQ2cE-vjhoobD_BYbTEhYROzVYY_mqdWuHYuvRGeD531d7k0AHWrhuLWbSjnEHIkDRltluTVgD8XcPRSUjrky0-QjZaLHdqJc6jiKb6SplK55pxh9yAXINZlfUQd1WXR-lF5vXs4kATf3OWFMuBfdAwmRMaeaBqmHPfbtvBe0oHSTqKnW3k_QAo_Cit124S90djd-U4DG1-g46Xh1plLA1WY1Wy4lvssAuBBvyKuZSZRIxIfNuDRu21lNjeM2lwFnb91CrDfJdKwlBgZ4sVVh7ELsEIlgJKOY_O8UNLqmq_7fyhg4ap-CdD6rXIt1OSc-5aJwLf5EZZ66N-EJFLx0uRhwCDRQPSoJac6-89VRQBHKSy1mFH1-G9E-h0Ti8dWpeAplBGwhsuohRedGkmeFBIe_MBWq8OAev82orN9W4bUOm5ulI5z-78MC2h8bgU_XnaYjVtVdf0LvBTRcUiqhqDJeF4g7QlBhynhZcIELBO3pLNCL1xyVqW1ARu_mtRjzjHT0YQ57r7F1rCDnJVFDUGbY-PFQlTao_IQoWJTlE0GvXKAdIRoZhUCYjz43ueJlCh5jE5zBRzAnvX69Ps2A3Rf9503AKeCnPA7X-DyVVIqKXlCdGao_jUruoep19DAK6RMhCg
Rename notebook

